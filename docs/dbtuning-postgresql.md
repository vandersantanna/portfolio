# üêò Complete PostgreSQL Performance Tuning Guide
*Database Performance Optimization - Professional Portfolio*

[![PostgreSQL](https://img.shields.io/badge/-PostgreSQL-336791?style=flat-square&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![AWS RDS](https://img.shields.io/badge/-AWS%20RDS-232F3E?style=flat-square&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/rds/)
[![Azure PostgreSQL](https://img.shields.io/badge/-Azure%20PostgreSQL-0078D4?style=flat-square&logo=microsoft-azure&logoColor=white)](https://azure.microsoft.com/services/postgresql/)
[![Google Cloud SQL](https://img.shields.io/badge/-Cloud%20SQL-4285F4?style=flat-square&logo=google-cloud&logoColor=white)](https://cloud.google.com/sql/)

> **Expert-level PostgreSQL performance optimization for mission-critical production environments**, including advanced configuration, query analysis, index tuning, and scalable solutions implementation across on-premise and cloud platforms.

---

## üìã Table of Contents

- [üéØ Objectives](#-objectives)
- [üîê Required Permissions](#-required-permissions)
- [üîç Performance Analysis](#-performance-analysis)
- [üìä PostgreSQL Index Types](#-postgresql-index-types)
- [‚öôÔ∏è Parameter Configuration](#Ô∏è-parameter-configuration)
- [üñ•Ô∏è On-Premise Tuning](#Ô∏è-on-premise-tuning)
- [‚òÅÔ∏è Cloud Tuning](#Ô∏è-cloud-tuning)
- [üîß Specialized Extensions](#-specialized-extensions)
- [üíº Practical Cases](#-practical-cases)
- [üìä Automated Monitoring](#-automated-monitoring)
- [üõ†Ô∏è Specialized Tools](#Ô∏è-specialized-tools)
- [üìö References](#-references)

---

## üéØ Objectives

### Performance Targets
- **Latency**: < 100ms for 95% of queries
- **Throughput**: > 10,000 TPS (Transactions Per Second)
- **Availability**: 99.9% uptime
- **Cache Hit Ratio**: > 99%
- **Index Usage**: > 95% of queries using indexes

### Key Performance Indicators (KPIs)
```sql
-- Primary KPIs for monitoring
SELECT 
    'Cache Hit Ratio' as metric,
    round(
        (sum(heap_blks_hit) / nullif(sum(heap_blks_hit + heap_blks_read), 0) * 100)::numeric, 2
    ) || '%' as value
FROM pg_statio_user_tables
UNION ALL
SELECT 
    'Index Usage Ratio' as metric,
    round(
        (sum(idx_scan) / nullif(sum(idx_scan + seq_scan), 0) * 100)::numeric, 2
    ) || '%' as value
FROM pg_stat_user_tables;
```

---

## üîê Required Permissions

### Monitoring User Setup
```sql
-- Create dedicated monitoring user
CREATE USER monitoring_user WITH PASSWORD 'secure_password';

-- Minimum required permissions
GRANT CONNECT ON DATABASE your_database TO monitoring_user;
GRANT USAGE ON SCHEMA public TO monitoring_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO monitoring_user;
GRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO monitoring_user;

-- System views access
GRANT SELECT ON pg_stat_database TO monitoring_user;
GRANT SELECT ON pg_stat_user_tables TO monitoring_user;
GRANT SELECT ON pg_stat_user_indexes TO monitoring_user;
GRANT SELECT ON pg_statio_user_tables TO monitoring_user;
GRANT SELECT ON pg_locks TO monitoring_user;
GRANT SELECT ON pg_stat_activity TO monitoring_user;

-- For pg_stat_statements
GRANT EXECUTE ON FUNCTION pg_stat_statements_reset() TO monitoring_user;
```

---

## üîç Performance Analysis

### 1. Identify Slow Queries
```sql
-- Top 10 slowest queries (requires pg_stat_statements)
SELECT 
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
ORDER BY total_exec_time DESC 
LIMIT 10;
```

### 2. Analyze Index Usage
```sql
-- Tables with low index usage
SELECT 
    schemaname,
    tablename,
    seq_scan,
    seq_tup_read,
    idx_scan,
    idx_tup_fetch,
    n_tup_ins + n_tup_upd + n_tup_del as total_writes,
    round(100 * idx_scan / nullif(seq_scan + idx_scan, 0), 2) as index_usage_ratio
FROM pg_stat_user_tables 
WHERE seq_scan > idx_scan
ORDER BY seq_scan DESC;
```

### 3. Identify Unused Indexes
```sql
-- Indexes that have never been used
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    pg_size_pretty(pg_relation_size(indexrelid)) as size
FROM pg_stat_user_indexes 
WHERE idx_scan = 0
AND NOT indisunique
ORDER BY pg_relation_size(indexrelid) DESC;
```

### 4. Detect Lock Contention
```sql
-- Active locks analysis
SELECT 
    pg_stat_activity.pid,
    pg_stat_activity.usename,
    pg_stat_activity.query_start,
    pg_stat_activity.state,
    pg_locks.locktype,
    pg_locks.mode,
    pg_locks.granted,
    pg_stat_activity.query
FROM pg_stat_activity 
JOIN pg_locks ON pg_stat_activity.pid = pg_locks.pid
WHERE NOT pg_locks.granted
ORDER BY pg_stat_activity.query_start;
```

### 5. Table Bloat Analysis
```sql
-- Detect table bloat (requires pgstattuple extension)
SELECT 
    tablename,
    pg_size_pretty(pg_total_relation_size(tablename::regclass)) as size,
    round((dead_tuple_percent)::numeric, 2) as bloat_percent,
    round((dead_tuple_count)::numeric, 0) as dead_tuples
FROM (
    SELECT 
        tablename,
        pgstattuple(tablename::regclass) as stats
    FROM pg_tables 
    WHERE schemaname = 'public'
) t,
LATERAL (
    SELECT 
        (stats).dead_tuple_percent,
        (stats).dead_tuple_count
) s
WHERE (stats).dead_tuple_percent > 10
ORDER BY (stats).dead_tuple_percent DESC;
```

### 6. Connection Monitoring
```sql
-- Active connections status
WITH connection_stats AS (
    SELECT 
        state,
        count(*) as connection_count,
        round(avg(extract(epoch from now() - query_start))::numeric, 2) as avg_duration_seconds
    FROM pg_stat_activity 
    WHERE pid != pg_backend_pid()
    GROUP BY state
)
SELECT 
    state,
    connection_count,
    avg_duration_seconds,
    round(100.0 * connection_count / sum(connection_count) OVER (), 2) as percentage
FROM connection_stats
ORDER BY connection_count DESC;
```

---

## üìä PostgreSQL Index Types

### 1. B-tree (Default)
```sql
-- Standard index for equality and range searches
CREATE INDEX idx_users_email ON users USING btree (email);
CREATE INDEX idx_orders_date_range ON orders USING btree (created_at, status);

-- Composite index with specific ordering
CREATE INDEX idx_products_category_price ON products (category_id, price DESC);
```

### 2. Hash Index
```sql
-- Ideal for exact equality searches
CREATE INDEX idx_users_uuid ON users USING hash (uuid);
-- Note: Only for = operator, doesn't support range queries
```

### 3. GIN (Generalized Inverted Index)
```sql
-- For arrays, JSONB and full-text search
CREATE INDEX idx_tags_gin ON articles USING gin (tags);
CREATE INDEX idx_metadata_gin ON products USING gin (metadata jsonb_ops);

-- Full-text search
CREATE INDEX idx_articles_fts ON articles USING gin (to_tsvector('english', title || ' ' || content));
```

### 4. GiST (Generalized Search Tree)
```sql
-- For geometric data and full-text search
CREATE INDEX idx_locations_gist ON stores USING gist (coordinates);
CREATE INDEX idx_articles_gist ON articles USING gist (to_tsvector('english', content));
```

### 5. BRIN (Block Range INdexes)
```sql
-- For very large tables with naturally ordered data
CREATE INDEX idx_logs_timestamp_brin ON logs USING brin (timestamp);
-- Ideal for time-series data
```

### 6. Partial Index
```sql
-- Index only for subset of data
CREATE INDEX idx_active_users ON users (email) WHERE active = true;
CREATE INDEX idx_pending_orders ON orders (created_at) WHERE status = 'pending';
```

### 7. Expression Index
```sql
-- Index on calculated expressions
CREATE INDEX idx_users_lower_email ON users (lower(email));
CREATE INDEX idx_orders_month ON orders (date_trunc('month', created_at));
```

---

## ‚öôÔ∏è Parameter Configuration

### Critical Parameters by Scenario

#### OLTP (Online Transaction Processing)
```ini
# OLTP optimized configuration
shared_buffers = 25% of RAM
work_mem = 16MB
maintenance_work_mem = 1GB
effective_cache_size = 75% of RAM
random_page_cost = 1.1        # SSD
seq_page_cost = 1.0
checkpoint_completion_target = 0.9
wal_buffers = 64MB
max_connections = 200
```

#### OLAP (Online Analytical Processing)
```ini
# OLAP/Data Warehouse optimized configuration
shared_buffers = 25% of RAM
work_mem = 256MB              # Higher for complex sorts/joins
maintenance_work_mem = 4GB    # For heavy maintenance operations
effective_cache_size = 75% of RAM
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_worker_processes = 12
```

### Automatic Parameter Calculation
```sql
-- Script to generate hardware-based configurations
WITH system_info AS (
    SELECT 
        setting as max_connections
    FROM pg_settings 
    WHERE name = 'max_connections'
),
memory_calc AS (
    SELECT 
        -- Simulate 64GB RAM
        68719476736 as total_memory_bytes,
        68719476736 * 0.25 as shared_buffers_bytes,
        68719476736 * 0.75 as effective_cache_size_bytes
)
SELECT 
    'shared_buffers' as parameter,
    pg_size_pretty(shared_buffers_bytes::bigint) as recommended_value
FROM memory_calc
UNION ALL
SELECT 
    'effective_cache_size' as parameter,
    pg_size_pretty(effective_cache_size_bytes::bigint) as recommended_value
FROM memory_calc;
```

---

## üñ•Ô∏è On-Premise Tuning

### Optimized postgresql.conf (64GB RAM)
```ini
# ===== CONNECTIONS =====
listen_addresses = '*'
port = 5432
max_connections = 200
superuser_reserved_connections = 3

# ===== MEMORY =====
shared_buffers = 16GB                   # 25% of RAM
huge_pages = try
temp_buffers = 32MB
work_mem = 64MB                         # Adjust based on workload
maintenance_work_mem = 2GB
autovacuum_work_mem = -1

# ===== WAL =====
wal_level = replica
fsync = on
synchronous_commit = on
wal_compression = on
wal_buffers = 64MB
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9
max_wal_size = 8GB
min_wal_size = 2GB

# ===== QUERY PLANNER =====
random_page_cost = 1.1                  # SSD
seq_page_cost = 1.0
effective_cache_size = 48GB              # 75% of RAM
default_statistics_target = 500

# ===== PARALLELISM =====
max_worker_processes = 12
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4

# ===== VACUUM =====
autovacuum = on
autovacuum_max_workers = 6
autovacuum_naptime = 30s
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05

# ===== LOGGING =====
log_destination = 'csvlog'
logging_collector = on
log_directory = '/var/log/postgresql'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_min_duration_statement = 1000       # Log queries > 1s
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0

# ===== EXTENSIONS =====
shared_preload_libraries = 'pg_stat_statements,auto_explain,pg_hint_plan'
```

### Operating System Optimization
```bash
#!/bin/bash
# OS optimization script for PostgreSQL

# Kernel configurations
cat >> /etc/sysctl.conf << EOF
# PostgreSQL optimizations
kernel.shmmax = 68719476736              # 64GB
kernel.shmall = 16777216                 # shmmax/4096
kernel.shmmni = 4096
fs.file-max = 65536
vm.overcommit_memory = 2
vm.overcommit_ratio = 90
vm.swappiness = 1
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
vm.dirty_writeback_centisecs = 100
vm.dirty_expire_centisecs = 500
EOF

# Apply configurations
sysctl -p

# I/O scheduler for SSD
echo noop > /sys/block/nvme0n1/queue/scheduler

# File limits
cat >> /etc/security/limits.conf << EOF
* soft nofile 65536
* hard nofile 65536
postgres soft nofile 65536
postgres hard nofile 65536
EOF

# Huge pages
echo 'vm.nr_hugepages = 8192' >> /etc/sysctl.conf  # For 16GB shared_buffers
```

---

## ‚òÅÔ∏è Cloud Tuning

### AWS RDS PostgreSQL

#### Parameter Group Configuration
```bash
# Create custom parameter group
aws rds create-db-parameter-group \
    --db-parameter-group-name postgresql-optimized-14 \
    --db-parameter-group-family postgres14 \
    --description "PostgreSQL 14 optimized for production"

# Apply optimized parameters
aws rds modify-db-parameter-group \
    --db-parameter-group-name postgresql-optimized-14 \
    --parameters \
        ParameterName=shared_buffers,ParameterValue="{DBInstanceClassMemory/4}" \
        ParameterName=effective_cache_size,ParameterValue="{DBInstanceClassMemory*3/4}" \
        ParameterName=work_mem,ParameterValue=32768 \
        ParameterName=maintenance_work_mem,ParameterValue=1048576 \
        ParameterName=checkpoint_completion_target,ParameterValue=0.9 \
        ParameterName=wal_buffers,ParameterValue=2048 \
        ParameterName=default_statistics_target,ParameterValue=200 \
        ParameterName=random_page_cost,ParameterValue=1.1 \
        ParameterName=log_min_duration_statement,ParameterValue=1000 \
        ParameterName=shared_preload_libraries,ParameterValue="pg_stat_statements"
```

#### CloudWatch Monitoring
```python
import boto3
import json

def create_postgresql_dashboard():
    cloudwatch = boto3.client('cloudwatch')
    
    dashboard_body = {
        "widgets": [
            {
                "type": "metric",
                "properties": {
                    "metrics": [
                        ["AWS/RDS", "CPUUtilization", "DBInstanceIdentifier", "your-db-instance"],
                        [".", "DatabaseConnections", ".", "."],
                        [".", "FreeableMemory", ".", "."],
                        [".", "ReadLatency", ".", "."],
                        [".", "WriteLatency", ".", "."]
                    ],
                    "period": 300,
                    "stat": "Average",
                    "region": "us-east-1",
                    "title": "PostgreSQL Performance Metrics"
                }
            }
        ]
    }
    
    cloudwatch.put_dashboard(
        DashboardName='PostgreSQL-Performance',
        DashboardBody=json.dumps(dashboard_body)
    )
```

### Azure Database for PostgreSQL
```bash
# Configuration via Azure CLI
az postgres server configuration set \
    --resource-group myResourceGroup \
    --server-name myserver \
    --name shared_buffers \
    --value 2097152

az postgres server configuration set \
    --resource-group myResourceGroup \
    --server-name myserver \
    --name effective_cache_size \
    --value 6291456
```

### Google Cloud SQL PostgreSQL
```yaml
# Terraform configuration for Cloud SQL
resource "google_sql_database_instance" "postgres" {
  name             = "postgres-optimized"
  database_version = "POSTGRES_14"
  region          = "us-central1"

  settings {
    tier = "db-custom-8-32768"  # 8 vCPUs, 32GB RAM
    
    database_flags {
      name  = "shared_buffers"
      value = "8388608"  # 8GB in 8KB pages
    }
    
    database_flags {
      name  = "effective_cache_size"
      value = "25165824"  # 24GB in 8KB pages
    }
    
    database_flags {
      name  = "work_mem"
      value = "32768"  # 32MB in KB
    }
    
    database_flags {
      name  = "random_page_cost"
      value = "1.1"
    }
  }
}
```

---

## üîß Specialized Extensions

### 1. pg_stat_statements
```sql
-- Install and configure
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Configure in postgresql.conf
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
pg_stat_statements.track_utility = on
pg_stat_statements.save = on

-- Analyze most expensive queries
SELECT 
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    stddev_exec_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
WHERE query NOT LIKE '%pg_stat_statements%'
ORDER BY total_exec_time DESC 
LIMIT 20;
```

### 2. auto_explain
```sql
-- Configure in postgresql.conf
shared_preload_libraries = 'auto_explain'
auto_explain.log_min_duration = 1000    # Log plans for queries > 1s
auto_explain.log_analyze = on
auto_explain.log_buffers = on
auto_explain.log_timing = on
auto_explain.log_verbose = on
auto_explain.log_nested_statements = on
auto_explain.log_format = 'json'
```

### 3. pg_hint_plan
```sql
-- Manual control of execution plans
CREATE EXTENSION IF NOT EXISTS pg_hint_plan;

-- Usage example
/*+
    SeqScan(large_table)
    NestLoop(orders customers)
    IndexScan(orders idx_orders_customer_id)
*/
SELECT o.*, c.name 
FROM orders o 
JOIN customers c ON o.customer_id = c.id 
WHERE o.created_at > '2024-01-01';
```

### 4. pgstattuple
```sql
-- Detailed bloat analysis
CREATE EXTENSION IF NOT EXISTS pgstattuple;

-- Analyze table bloat
SELECT 
    table_len,
    tuple_count,
    tuple_len,
    tuple_percent,
    dead_tuple_count,
    dead_tuple_len,
    dead_tuple_percent,
    free_space,
    free_percent
FROM pgstattuple('large_table');

-- Analyze index bloat
SELECT 
    version,
    tree_level,
    index_size,
    root_block_no,
    internal_pages,
    leaf_pages,
    empty_pages,
    deleted_pages,
    avg_leaf_density,
    leaf_fragmentation
FROM pgstatindex('idx_large_table_id');
```

### 5. pg_buffercache
```sql
-- Analyze buffer cache contents
CREATE EXTENSION IF NOT EXISTS pg_buffercache;

-- Top tables in cache
SELECT 
    c.relname,
    count(*) as buffers,
    round(100.0 * count(*) / (SELECT setting FROM pg_settings WHERE name='shared_buffers')::integer, 2) as buffer_percent,
    round(avg(usagecount), 2) as avg_usage
FROM pg_buffercache b
INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid)
WHERE b.relfilenode IS NOT NULL
GROUP BY c.relname
ORDER BY buffers DESC
LIMIT 20;
```

### 6. pg_prewarm
```sql
-- Preload important data into cache
CREATE EXTENSION IF NOT EXISTS pg_prewarm;

-- Prewarm critical table
SELECT pg_prewarm('critical_table');

-- Prewarm specific index
SELECT pg_prewarm('idx_critical_table_id');

-- Configure automatic prewarming
-- In postgresql.conf:
shared_preload_libraries = 'pg_prewarm'
pg_prewarm.autoprewarm = on
pg_prewarm.autoprewarm_interval = 300
```

---

## üíº Practical Cases

### Case 1: High-Volume E-commerce Platform

#### Problem
- 50,000 orders/day
- Reporting queries affecting OLTP performance
- Cache hit ratio at 85%

#### Implemented Solution
```sql
-- 1. Workload separation (Read Replica)
-- Master/slave configuration to separate OLTP from reporting

-- 2. Critical index optimization
CREATE INDEX CONCURRENTLY idx_orders_status_created 
ON orders (status, created_at) 
WHERE status IN ('pending', 'processing');

CREATE INDEX CONCURRENTLY idx_order_items_product_qty 
ON order_items (product_id, quantity) 
WHERE quantity > 0;

-- 3. Date-based partitioning
CREATE TABLE orders_2024_01 PARTITION OF orders 
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

-- 4. Optimized configuration
shared_buffers = 8GB
work_mem = 32MB
effective_cache_size = 24GB
```

#### Results
- **Cache hit ratio**: 85% ‚Üí 99.2%
- **Average latency**: 250ms ‚Üí 45ms
- **Throughput**: 2,500 TPS ‚Üí 8,500 TPS

### Case 2: Data Warehouse with Complex Analytical Queries

#### Problem
- Report queries running for hours
- Frequent locks during ETL
- Excessive CPU usage

#### Implemented Solution
```sql
-- 1. Specialized indexes for analytics
CREATE INDEX CONCURRENTLY idx_sales_date_gin 
ON sales USING gin (date_trunc('month', sale_date));

CREATE INDEX CONCURRENTLY idx_products_category_brin 
ON products USING brin (category_id, created_at);

-- 2. Critical view materialization
CREATE MATERIALIZED VIEW sales_monthly_summary AS
SELECT 
    date_trunc('month', sale_date) as month,
    category_id,
    sum(amount) as total_sales,
    count(*) as total_orders,
    avg(amount) as avg_order_value
FROM sales s
JOIN products p ON s.product_id = p.id
GROUP BY 1, 2;

CREATE UNIQUE INDEX ON sales_monthly_summary (month, category_id);

-- 3. Configuration for analytical workload
work_mem = 256MB
maintenance_work_mem = 4GB
max_parallel_workers_per_gather = 6
```

#### Results
- **Complex query**: 4h ‚Üí 15min
- **ETL performance**: 300% improvement
- **Parallelization**: queries using up to 6 workers

### Case 3: Multi-tenant SaaS Application

#### Problem
- Performance degrading with tenant growth
- Cross-tenant queries causing lock contention
- Need for performance isolation

#### Implemented Solution
```sql
-- 1. Optimized Row Level Security
CREATE POLICY tenant_isolation ON users 
FOR ALL TO app_role 
USING (tenant_id = current_setting('app.current_tenant')::integer);

-- 2. Tenant-specific indexes
CREATE INDEX CONCURRENTLY idx_users_tenant_email 
ON users (tenant_id, email) 
WHERE active = true;

-- 3. Per-tenant connection pooling
-- PgBouncer configuration
[databases]
tenant_1 = host=localhost dbname=app user=app_user pool_size=25
tenant_2 = host=localhost dbname=app user=app_user pool_size=25

-- 4. Per-tenant monitoring
CREATE VIEW tenant_performance AS
SELECT 
    current_setting('app.current_tenant') as tenant_id,
    schemaname,
    tablename,
    seq_scan,
    idx_scan,
    n_tup_ins + n_tup_upd + n_tup_del as total_writes
FROM pg_stat_user_tables;
```

#### Results
- **Isolation**: 100% between tenants
- **Performance**: consistent regardless of tenant count
- **Scaling**: support for 500+ active tenants

---

## üìä Automated Monitoring

### Complete Monitoring Script
```python
#!/usr/bin/env python3
"""
PostgreSQL Performance Monitor
Collects performance metrics and generates alerts
"""

import psycopg2
import json
import smtplib
from datetime import datetime
from email.mime.text import MIMEText

class PostgreSQLMonitor:
    def __init__(self, connection_params):
        self.conn_params = connection_params
        self.alerts = []
    
    def connect(self):
        return psycopg2.connect(**self.conn_params)
    
    def check_cache_hit_ratio(self):
        """Check cache hit ratio - alert if < 95%"""
        query = """
        SELECT round(
            (sum(heap_blks_hit) / nullif(sum(heap_blks_hit + heap_blks_read), 0) * 100)::numeric, 2
        ) as cache_hit_ratio
        FROM pg_statio_user_tables;
        """
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                cur.execute(query)
                ratio = cur.fetchone()[0]
                
                if ratio < 95:
                    self.alerts.append({
                        'metric': 'cache_hit_ratio',
                        'value': ratio,
                        'threshold': 95,
                        'severity': 'warning' if ratio > 90 else 'critical'
                    })
                
                return ratio
    
    def check_slow_queries(self):
        """Identify slow queries - alert if > 5s average"""
        query = """
        SELECT 
            query,
            calls,
            mean_exec_time,
            total_exec_time
        FROM pg_stat_statements 
        WHERE mean_exec_time > 5000
        ORDER BY mean_exec_time DESC 
        LIMIT 10;
        """
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                cur.execute(query)
                slow_queries = cur.fetchall()
                
                if slow_queries:
                    self.alerts.append({
                        'metric': 'slow_queries',
                        'value': len(slow_queries),
                        'queries': slow_queries,
                        'severity': 'warning'
                    })
                
                return slow_queries
    
    def check_connections(self):
        """Monitor number of active connections"""
        query = """
        SELECT 
            count(*) as active_connections,
            (SELECT setting::int FROM pg_settings WHERE name = 'max_connections') as max_connections
        FROM pg_stat_activity 
        WHERE state = 'active';
        """
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                cur.execute(query)
                active, max_conn = cur.fetchone()
                
                usage_percent = (active / max_conn) * 100
                
                if usage_percent > 80:
                    self.alerts.append({
                        'metric': 'connection_usage',
                        'value': usage_percent,
                        'active_connections': active,
                        'max_connections': max_conn,
                        'severity': 'critical' if usage_percent > 90 else 'warning'
                    })
                
                return active, max_conn
    
    def check_database_size(self):
        """Monitor database growth"""
        query = """
        SELECT 
            pg_database.datname,
            pg_size_pretty(pg_database_size(pg_database.datname)) as size,
            pg_database_size(pg_database.datname) as size_bytes
        FROM pg_database 
        WHERE pg_database.datname NOT IN ('template0', 'template1', 'postgres')
        ORDER BY pg_database_size(pg_database.datname) DESC;
        """
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                cur.execute(query)
                return cur.fetchall()
    
    def check_locks(self):
        """Check for blocking locks"""
        query = """
        SELECT 
            blocked_locks.pid AS blocked_pid,
            blocked_activity.usename AS blocked_user,
            blocking_locks.pid AS blocking_pid,
            blocking_activity.usename AS blocking_user,
            blocked_activity.query AS blocked_statement,
            blocking_activity.query AS current_statement_in_blocking_process,
            blocked_activity.application_name AS blocked_application,
            blocking_activity.application_name AS blocking_application
        FROM pg_catalog.pg_locks blocked_locks
        JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
        JOIN pg_catalog.pg_locks blocking_locks 
            ON blocking_locks.locktype = blocked_locks.locktype
            AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
            AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
            AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
            AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
            AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
            AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
            AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
            AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
            AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
            AND blocking_locks.pid != blocked_locks.pid
        JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
        WHERE NOT blocked_locks.granted;
        """
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                cur.execute(query)
                locks = cur.fetchall()
                
                if locks:
                    self.alerts.append({
                        'metric': 'blocking_locks',
                        'value': len(locks),
                        'locks': locks,
                        'severity': 'critical'
                    })
                
                return locks
    
    def generate_report(self):
        """Generate complete performance report"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'cache_hit_ratio': self.check_cache_hit_ratio(),
            'slow_queries': self.check_slow_queries(),
            'connections': self.check_connections(),
            'database_sizes': self.check_database_size(),
            'blocking_locks': self.check_locks(),
            'alerts': self.alerts
        }
        
        return report
    
    def send_alerts(self, smtp_config, recipients):
        """Send alerts via email if critical issues found"""
        critical_alerts = [a for a in self.alerts if a['severity'] == 'critical']
        
        if critical_alerts:
            subject = f"PostgreSQL Critical Alert - {len(critical_alerts)} issues"
            body = "Critical PostgreSQL issues detected:\n\n"
            
            for alert in critical_alerts:
                body += f"- {alert['metric']}: {alert['value']}\n"
            
            msg = MIMEText(body)
            msg['Subject'] = subject
            msg['From'] = smtp_config['from']
            msg['To'] = ', '.join(recipients)
            
            with smtplib.SMTP(smtp_config['server'], smtp_config['port']) as server:
                server.starttls()
                server.login(smtp_config['username'], smtp_config['password'])
                server.send_message(msg)

# Usage example
if __name__ == "__main__":
    # Connection configuration
    conn_params = {
        'host': 'localhost',
        'database': 'production',
        'user': 'monitoring_user',
        'password': 'secure_password'
    }
    
    # SMTP configuration
    smtp_config = {
        'server': 'smtp.gmail.com',
        'port': 587,
        'username': 'alerts@company.com',
        'password': 'app_password',
        'from': 'alerts@company.com'
    }
    
    # Execute monitoring
    monitor = PostgreSQLMonitor(conn_params)
    report = monitor.generate_report()
    
    # Save report
    with open(f"pg_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    # Send alerts if necessary
    if monitor.alerts:
        monitor.send_alerts(smtp_config, ['dba@company.com', 'ops@company.com'])
    
    print(f"Report generated. {len(monitor.alerts)} alerts found.")
```

### Grafana Dashboard for PostgreSQL
```json
{
  "dashboard": {
    "title": "PostgreSQL Performance Dashboard",
    "panels": [
      {
        "title": "Cache Hit Ratio",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(pg_stat_database_blks_hit[5m]) / (rate(pg_stat_database_blks_hit[5m]) + rate(pg_stat_database_blks_read[5m])) * 100",
            "legendFormat": "Cache Hit %"
          }
        ],
        "thresholds": [
          {"color": "red", "value": 0},
          {"color": "yellow", "value": 95},
          {"color": "green", "value": 99}
        ]
      },
      {
        "title": "Active Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_activity_count",
            "legendFormat": "Active Connections"
          }
        ]
      },
      {
        "title": "Query Duration",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(pg_stat_statements_total_time[5m]) / rate(pg_stat_statements_calls[5m])",
            "legendFormat": "Avg Query Time (ms)"
          }
        ]
      }
    ]
  }
}
```

### Automated Backup Script with Performance Check
```bash
#!/bin/bash
# PostgreSQL Backup with Performance Check

DB_NAME="production"
BACKUP_DIR="/backups/postgresql"
LOG_FILE="/var/log/postgresql/backup.log"
RETENTION_DAYS=7

# Function to log with timestamp
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

# Check performance before backup
check_performance() {
    log "Checking database performance..."
    
    CACHE_HIT=$(psql -d $DB_NAME -t -c "
        SELECT round(
            (sum(heap_blks_hit) / nullif(sum(heap_blks_hit + heap_blks_read), 0) * 100)::numeric, 2
        ) FROM pg_statio_user_tables;"
    )
    
    ACTIVE_CONN=$(psql -d $DB_NAME -t -c "
        SELECT count(*) FROM pg_stat_activity WHERE state = 'active';"
    )
    
    log "Cache Hit Ratio: ${CACHE_HIT}%"
    log "Active Connections: $ACTIVE_CONN"
    
    # Alert if poor performance
    if (( $(echo "$CACHE_HIT < 95" | bc -l) )); then
        log "WARNING: Low cache hit ratio: ${CACHE_HIT}%"
    fi
    
    if [ "$ACTIVE_CONN" -gt 100 ]; then
        log "WARNING: High number of active connections: $ACTIVE_CONN"
    fi
}

# Perform backup
perform_backup() {
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    BACKUP_FILE="${BACKUP_DIR}/backup_${DB_NAME}_${TIMESTAMP}.sql"
    
    log "Starting backup to $BACKUP_FILE"
    
    pg_dump -h localhost -U backup_user -d $DB_NAME \
        --verbose \
        --no-password \
        --format=custom \
        --compress=9 \
        --file="$BACKUP_FILE" 2>&1 | tee -a $LOG_FILE
    
    if [ ${PIPESTATUS[0]} -eq 0 ]; then
        log "Backup completed successfully"
        
        # Compress backup
        gzip "$BACKUP_FILE"
        log "Backup compressed: ${BACKUP_FILE}.gz"
        
        # Verify integrity
        pg_restore --list "${BACKUP_FILE}.gz" > /dev/null 2>&1
        if [ $? -eq 0 ]; then
            log "Backup integrity verified"
        else
            log "ERROR: Backup integrity check failed"
            exit 1
        fi
    else
        log "ERROR: Backup failed"
        exit 1
    fi
}

# Cleanup old backups
cleanup_old_backups() {
    log "Cleaning up backups older than $RETENTION_DAYS days"
    find $BACKUP_DIR -name "backup_${DB_NAME}_*.sql.gz" -mtime +$RETENTION_DAYS -delete
    log "Cleanup completed"
}

# Execute complete process
main() {
    log "=== PostgreSQL Backup Process Started ==="
    
    check_performance
    perform_backup
    cleanup_old_backups
    
    log "=== PostgreSQL Backup Process Completed ==="
}

# Execute if called directly
if [ "${BASH_SOURCE[0]}" == "${0}" ]; then
    main "$@"
fi
```

---

## üõ†Ô∏è Specialized Tools

### Analysis Tools

| Tool | Category | Description | Primary Use |
|------|----------|-------------|-------------|
| **pgBadger** | Log Analysis | PostgreSQL log analyzer | Detailed performance reports |
| **pg_stat_statements** | Query Analysis | Extension for query statistics | Identifying problematic queries |
| **EXPLAIN ANALYZE** | Query Planning | Execution plan analyzer | Specific query optimization |
| **pgBench** | Benchmarking | Official benchmark tool | Load and performance testing |
| **pg_activity** | Real-time Monitor | Real-time monitor | Current activity visualization |

### Monitoring Tools

| Tool | Type | Characteristics | Best For |
|------|------|-----------------|----------|
| **Prometheus + pg_exporter** | Metrics | Time-series metrics collection | Alerts and dashboards |
| **Grafana** | Visualization | Customizable dashboards | Metrics visualization |
| **pgWatch2** | All-in-one | Complete monitoring solution | Small/medium environments |
| **DataDog** | SaaS | Cloud monitoring | Companies with budget |
| **New Relic** | APM | Application Performance Monitoring | Application + DB analysis |

### Optimization Tools

| Tool | Functionality | Description | Use Case |
|------|---------------|-------------|----------|
| **pg_repack** | Reorganization | Reorganizes tables without locks | Production bloat reduction |
| **pgTune** | Config Generator | Generates optimized configurations | Initial configuration |
| **PoWA** | Workload Analyzer | Analyzes complete workload | Continuous optimization |
| **pg_stat_kcache** | Kernel Stats | Kernel statistics | I/O performance |
| **pg_qualstats** | Predicate Analysis | Analyzes WHERE predicates | Index suggestions |

### Backup and Recovery Tools

| Tool | Type | Characteristics | Advantages |
|------|------|-----------------|------------|
| **pgBackRest** | Backup/Restore | Incremental and parallel backup | Performance and reliability |
| **Barman** | Backup Manager | Complete backup management | Point-in-time recovery |
| **pg_dump/pg_restore** | Built-in | Native tools | Simplicity and compatibility |
| **WAL-E/WAL-G** | Continuous Archive | Continuous WAL archiving | Cloud storage backup |

### Migration Tools

| Tool | Functionality | Source ‚Üí Target | Use Cases |
|------|---------------|-----------------|-----------|
| **pg_upgrade** | Version Upgrade | PostgreSQL ‚Üí PostgreSQL (new version) | Version upgrade |
| **pglogical** | Logical Replication | PostgreSQL ‚Üí PostgreSQL | Minimal downtime migration |
| **AWS DMS** | Data Migration | Multiple sources ‚Üí PostgreSQL | AWS migration |
| **ora2pg** | Oracle Migration | Oracle ‚Üí PostgreSQL | Oracle migration |
| **mysql2postgresql** | MySQL Migration | MySQL ‚Üí PostgreSQL | MySQL migration |

### Development Tools

| Tool | Category | Description | Benefits |
|------|----------|-------------|----------|
| **pgAdmin** | GUI Admin | Complete graphical interface | Visual administration |
| **DBeaver** | Multi-DB Client | Universal database client | Development and analysis |
| **pgModeler** | Modeling | Visual ERD modeling | Schema design |
| **Flyway** | Migration | Schema version control | Database DevOps |
| **Liquibase** | Change Management | Change management | Change control |

---

## üìö References

### Official Documentation
- [PostgreSQL Documentation](https://www.postgresql.org/docs/) - Complete official documentation
- [PostgreSQL Wiki](https://wiki.postgresql.org/) - Community wiki
- [PostgreSQL Performance Tips](https://wiki.postgresql.org/wiki/Performance_Optimization) - Official performance tips

### Specialized Books
- **"PostgreSQL High Performance"** - Gregory Smith
- **"PostgreSQL Administration Cookbook"** - Simon Riggs & Gianni Ciolli
- **"Mastering PostgreSQL"** - Hans-J√ºrgen Sch√∂nig
- **"PostgreSQL Query Optimization"** - Henrietta Dombrovskaya

### Blogs and Technical Resources
- [PostgreSQL Planet](https://planet.postgresql.org/) - Community blog aggregator
- [2ndQuadrant Blog](https://www.2ndquadrant.com/en/blog/) - Specialized technical blog
- [Percona PostgreSQL Blog](https://www.percona.com/blog/tag/postgresql/) - Performance and optimization
- [Citus Data Blog](https://www.citusdata.com/blog/) - Scalability and distributed PostgreSQL

### Tools and Utilities
- [pgTune](https://pgtune.leopard.in.ua/) - Configuration generator
- [EXPLAIN Visualizer](https://tatiyants.com/pev/) - Execution plan visualizer
- [pgBadger](https://github.com/darold/pgbadger) - Log analyzer
- [pg_activity](https://github.com/dalibo/pg_activity) - Real-time monitor

### Community and Support
- [PostgreSQL Mailing Lists](https://www.postgresql.org/list/) - Official discussion lists
- [Stack Overflow PostgreSQL](https://stackoverflow.com/questions/tagged/postgresql) - Community Q&A
- [Reddit r/PostgreSQL](https://www.reddit.com/r/PostgreSQL/) - Informal discussions
- [PostgreSQL Slack](https://postgres-slack.herokuapp.com/) - Community chat

### Certifications and Training
- [PostgreSQL Certification](https://www.postgresql.org/about/policies/certification/) - Official program
- [2ndQuadrant Training](https://www.2ndquadrant.com/en/training/) - Specialized training
- [Percona Training](https://www.percona.com/training) - Performance courses
- [pgDay/pgCon Conferences](https://www.postgresql.org/about/events/) - Technical conferences

---

## üë®‚Äçüíª About the Author

**PostgreSQL Performance Tuning Specialist** with expertise in:

### Technical Expertise
- **Database Administration**: 8+ years of PostgreSQL experience
- **Performance Optimization**: Tuning critical environments (> 100TB data)
- **Cloud Platforms**: AWS RDS, Azure Database, Google Cloud SQL
- **High Availability**: Streaming replication, Patroni, pgBouncer
- **Monitoring**: Prometheus, Grafana, pgWatch2, DataDog

### Featured Projects
- **E-commerce Platform**: Optimization from 15,000 TPS ‚Üí 45,000 TPS
- **Data Warehouse**: Query reduction from 4h ‚Üí 15min
- **Multi-tenant SaaS**: Per-tenant isolation implementation
- **Oracle ‚Üí PostgreSQL Migration**: 2TB data with zero downtime

### Certifications
- PostgreSQL Certified Professional
- AWS Database Specialty
- Google Cloud Professional Database Engineer

### Technologies
![PostgreSQL](https://img.shields.io/badge/-PostgreSQL-336791?style=flat-square&logo=postgresql&logoColor=white)
![Python](https://img.shields.io/badge/-Python-3776AB?style=flat-square&logo=python&logoColor=white)
![Bash](https://img.shields.io/badge/-Bash-4EAA25?style=flat-square&logo=gnu-bash&logoColor=white)
![Docker](https://img.shields.io/badge/-Docker-2496ED?style=flat-square&logo=docker&logoColor=white)
![Kubernetes](https://img.shields.io/badge/-Kubernetes-326CE5?style=flat-square&logo=kubernetes&logoColor=white)
![Terraform](https://img.shields.io/badge/-Terraform-623CE4?style=flat-square&logo=terraform&logoColor=white)

---

*This guide represents practical knowledge acquired in mission-critical production environments and is constantly updated as new versions and best practices emerge in the PostgreSQL community.*

**Last updated**: September 2025
**PostgreSQL version**: 14, 15, 16
**Tested environments**: On-premise, AWS RDS, Azure Database, Google Cloud SQL
